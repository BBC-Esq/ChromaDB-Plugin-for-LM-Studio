<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision Models</title>
	<style>
		body {
			font-family: Arial, sans-serif;
			line-height: 1.6;
			margin: 0;
			padding: 0;
			background-color: #161b22;
			color: #d0d0d0;
		}

		header {
			text-align: center;
			background-color: #3498db;
			color: #fff;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}

		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}

		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}

		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}

		p {
			text-indent: 35px;
		}

		table {
			border-collapse: collapse;
			width: 80%;
			margin: 50px auto;
		}

		th, td {
			text-align: left;
			padding: 8px;
			border-bottom: 1px solid #ddd;
		}

		th {
			background-color: #f2f2f2;
			color: #000;
		}

		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		code {
			background-color: #f9f9f9;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
	</style>

</head>

<body>
    <header>
        <h1>Vision Models in General</h1>
    </header>

    <main>
		
		<h2 style="color: #f0f0f0;" align="center">What are Vision Models?</h2>
		
		<p>Vision models are basically large language models that can analyze an image and either answers questions about it or
		describe it.  Vision models are used in this program to create detailed descriptions of multiple images and input each
		of those descriptions into a vector database to be searched.</p>
		
		<h2 style="color: #f0f0f0;" align="left">Types of Vision Models</h2>
		
		<p>This program uses four different vision models.</p>
				
		<p>The <code>llava</code> models were one of the first to become mainstream.  They come in 7b and 13b ("7b" meaning
		7 billion parameters) sizes and are based on the <code>llama2</code> architecture.  The <code>bakllava</code> model
		is based on the <code>mistral</code> architecture and has 7 billion parameters.</p>
		
		<p><code>cogvlm</code> has 18 billion parameters and is the largest and most resource-intensive model.  However, it also
		produces the highest quality image descriptions.</p>
		
		<p>Finally, the <code>moondream2</code> model is a recent addition and it by far strikes the best balance of quality
		and resource requirements.  For example, if you want 75% of the quality of <code>cogvlm</code> at 100x less processing
		time, use <code>moondream2</code>.</p>
		
		</p>Ignoring the compute requirements altogether, I personally rank the models' quality as follows.  My opinion takes
		into account both the "comprehensiveness" of the vision model's summary (i.e. how many things in the image did the
		vision model identify as important enough to describe) and "accuracy" (i.e. how many of those descriptions were
		actually accurate).  I highly recommend using the highest quality vision model you can.
		
		<ol>
			<li>cogvlm (9.5)</li>
			<li>bakllava (7.5)</li>
			<li>llava (7)</li>
			<li>moondream2 (6.5)</li>
		</ol>
		</p>

		<h2 style="color: #f0f0f0;" align="center">Vision Model Settings</h2>
		
		<p><code>Quant</code> or "quantization" refers to the floating point format of the model you want to use.  You can
		read more about "quantization" in general in the "Whisper" tab within the User Guide.  I've included multiple
		quality levels based on how a particular vision model was originally created as well as resource requirements - within
		reason.  For example, I have not included the option to run the <code>cogvlm</code> model in its original floating
		point format because it cannot fit on a consumer GPU having 24 GB of VRAM.</p>
		
		<p>The <code>4-bit</code> option used the well-known "bitsandbytes" library to quantize the model from its original
		floating point format, thus reducing the memory and compute requirements.  "Bitsandbytes" is only available for
		certain model architectures, however, but I've included it where possible.  There is a loss in quality when choosing
		<code>4-bit</code>, but most of the time it's minimal or, even if it isn't minimal, it's worth the tradeoff in compute
		time and resources.  I've personally found that running <code>cogvlm</code> with "4-bit" is still MUCH MUCH better than
		even running <code>bakllava</code> at "float16."</p>
		
		<p>Make sure and test your settings on a single image before committing 100 images to be processed.  This can be done
		within the "Tools Tab."</p>

		<h2 style="color: #f0f0f0;" align="center">Additional Tips</h2>
		
		<p>A greater number of parametrs generally improves quality more than choosing a smaller model with a larger quantization.
		The sole exception woudld be <code>moondream2</code>.  In my experience, this model at only 2 billion parameters, when
		running in <code>float16</code> produced better results than both <code>llava</code> and <code>bakllava</code> running
		in <code>4-bit</code>...desipte these latter two models having 7 billion parameters.</p>
		
		<p>Nothing can beat <code>cogvlm</code> in terms of quality.  Use it in <code>4-bit</code> if you have the resources
		and/or time.  Also, I do not recommend <code>8-bit</code> mode since there's a very small increase in quality in
		exchange for much larger compute time and resource requirements.</p>
		
		<p>Make sure your chunk size setting is at least as large as the longest summary that you expect the vision model you
		choose will create.  This is because you don't want to split one description into multiple chunks.  This is usually not
		an issue, however, since the descriptions are typically between 100-300 characters.  You can use the "Tools Tab" to
		get a sense of how longer your image descriptions will be.</p>

		<h2 style="color: #f0f0f0;" align="center">Tips</h2>
		
		<p>As always, before performing a transcription, proessing images or creating the vector database, "eject" the LLM
		from within LM Studio.  However, this is especially important when processing images with <code>cogvlm</code> because
		even in <code>4-bit</code> mode it requires a lot of resources.</p>
		
		<p>Make sure that the "chunk size" setting for creating the vector database is larger than the largest summary of an image
		that will be created.  The vision models typically create summaries ranging from 100-400 characters.  Therefore, if you
		specify a "chunk size" of at least 500-600 each summary will considered its own "chunk."  In other words, each summary
		of an image will be its own chunk instead and will not be split by the Recursive Character Text Splitter (discussed
		in the Settings tab).</p>

	</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
