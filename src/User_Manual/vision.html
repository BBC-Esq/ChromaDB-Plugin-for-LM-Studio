<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision Models</title>
	<style>
		body {
			font-family: Arial, sans-serif;
			line-height: 1.6;
			margin: 0;
			padding: 0;
			background-color: #161b22;
			color: #d0d0d0;
		}

		header {
			text-align: center;
			background-color: #3498db;
			color: #fff;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}

		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}

		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}

		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}

		p {
			text-indent: 35px;
		}

		table {
			border-collapse: collapse;
			width: 80%;
			margin: 50px auto;
		}

		th, td {
			text-align: left;
			padding: 8px;
			border-bottom: 1px solid #ddd;
		}

		th {
			background-color: #f2f2f2;
			color: #000;
		}

		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		code {
			background-color: #f9f9f9;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
	</style>

</head>

<body>
    <header>
        <h1>Vision</h1>
    </header>

    <main>

        <section>
		
		<h2 style="color: #f0f0f0;" align="center">What are Vision Models?</h2>
		
		<p>Vision models are basically large language models that can analyze and extract information from a variety of images.
		For purposes of this program, vision models are used to extract a summary of what an image depicts and add this description
		to the vector database where it can be searched along with any traditional documents you add!</p>
		
		<h2 style="color: #f0f0f0;" align="left">Which Vision Models Are Available?</h2>
		
		<p>This program allows you to choose betweeen four vision models.  The models are used to create summaries of images that you select, and these summaries are then put into the vector database when it is created.</p>
				
		<p><code>llava</code> models come in 7b and 13b ("7b" meaning 7 billion parameters) sizes and are based on the
		<code>llama2</code> architecture.  <code>bakllava</code> is based on the <code>mistral</code> architecture and
		comes in the 7b size.</p>
		
		<p><code>cogvlm</code> is the largest and most resource-intensive at <u>18b parameters</u> but also produces the highest
		quality results IMHO.  <code>moondream</code> is the newest vision model added and is almost as good as llava/bakllava with significantly less resource requirements.  <code>salesforce</code> is the fasted model with the lowest resource
		requirements, but it performs exceptionally well for its size (only 480m parameters).</p>
		
		</p>Setting aside for the moment speed and resource considerations...I personally ranked the vision models on a scale of 1-10
		to give you an idea of their quality.  My ranking score reflects "comprehensiveness" (i.e. how many things in the image
		did the vision model identify as important enough to try and describe) and "accuracy" (i.e. how many of those descriptions
		are, in fact, accurate):
		
		<ol>
			<li>cogvlm (9.5)</li>
			<li>bakllava (7.5)</li>
			<li>llava (7)</li>
			<li>moondream2 (6.5)</li>
			<li>salesforce (5)</li>
		</ol>
		</p>
		
		<h2 style="color: #f0f0f0;" align="center">Vision Model Settings</h2>
		
		<p><code>Model</code>:The model you want to use to create summaries of images.</p>
		
		<p><code>Size</code>:The number of parameters in millions or billions.  Informational purposes only; can't be changed.</p>
		
		<p><code>Quant</code> refers to the size of the model you want to run.  You can read more about "quantization" in the
		"Whisper" tab within the User Guide.  Salesforce was originally created in <code>float32</code> floating point format
		while the others were created with <code>float16</code>.  I've included the original floating point formats for all models
		except <code>cogvlm</code> because the resource requirements are too high (i.e. above 24 GB of memory).</p>
		
		<p><code>8-bit</code> and <code>4-bit</code> refer to quantization (using the "bitsandbytes" library) of the original floating point.  "Bitsandbytes" is only available for certain vision model architectures.  Choosing 8-bit or 4-bit will always
		result in less memory requirements as well as a small drop in accuracy, but for reasons I don't quite understand, this does not
		necessarily mean less processing time.  Just something to be aware of.</p>
		
		<p><code>Flash Attention 2</code> is a compelling newer library that reduces processing time and resource requirements
		significantly; however, I am still working on implementing it reliably so it is currently disabled.</p>
		
		<p><code>Batch</code>: This will be enabled and explained in future releases.</p>
		
		<h2 style="color: #f0f0f0;" align="center">How do I use the Vision Model?</h2>
		
		<p>Within the <code>Create Database</code> you can select files to be included in the vector database, including images.
		When you click the create database button, the vision models will create summaries of each image and put them into the
		vector database.  The particular vision model used and its settings can be controlled from the <code>Settings</code> tab.
		In contrast to adding other types of files to the vector database, obtaining summaries of images is typically much more
		time consuming and resource intensive depending on the vision model and settings you choose.  Therefore, I recommend
		first testing the various vision models on a single image in the <code>Tools</code> tab.</p>
		
		<p>In the <code>Tools</code> tab, select a single image and click "process."  This will use the vision model and settings chosen within the <code>Settings</code> tab to process a single image and display the summary for you.  Thus, you can see how long it took as well as the quality of the summary.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Tips</h2>
		
		<p>Use the highest quality vision model that can be run entirely from VRAM, unloading the model from LM Studio beforehand
		if need be.  If you want the best balance of speed and quality, <code>moondream2</code> is the best IMHO.</p>
		
		<p>Make sure that the "chunk size" setting for creating the vector database is larger than the largest summary of an image
		that will be created.  The vision models typically create summaries ranging from 100-400 characters.  Therefore, if you
		specify a "chunk size" of at least 500-600 each summary will considered its own "chunk."  In other words, each summary
		of an image will be its own chunk instead and will not be split by the Recursive Character Text Splitter (discussed
		in the Settings tab).</p>

	</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
