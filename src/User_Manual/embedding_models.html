<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embedding Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #161b22;
            color: #d0d0d0;
        }
		
		header {
			text-align: center;
			background-color: #3498db;
			color: #333;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}
		
		header a {
			color: #fff;
			text-decoration: none;
		}
		
		h2:target {
			padding-top: 200px;
			margin-top: -100px;
			display: block;
		}

		header a:hover {
			text-decoration: underline;
		}
		
		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}
		
		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}
		
		p {
			text-indent: 25px;
		}

		
		table {
            color: black;
			border-collapse: collapse;
            margin: 25px auto;
        }
		
		thead th {
			background-color: #f69784;
		}

        table, th, td {
            border: 1px solid black;
        }

        th, td {
            padding: 8px 12px;
        }

        .tiny {
            background-color: #e6f7ff;
        }

        .base {
            background-color: #b3e0ff;
        }

        .small {
            background-color: #66c2ff;
        }

        .medium {
            background-color: #3399ff;
        }

        .large {
            background-color: #0073e6;
        }
		
		code {
			background-color: #d0d0d0;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
		
		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		.table-from-second-file, .table-from-second-file th, .table-from-second-file td {
			border-collapse: collapse;
			margin: 25px auto;
			text-align: center;
			padding: 8px;
			border-bottom: 1px solid #ddd;
			color: #000;
		}

		.table-from-second-file th {
			background-color: #f2f2f2;
			color: #000;
		}
		
		.table-from-second-file td {
			color: #fff; /* White color for non-header cells */
			border-bottom: 1px solid #fff;
			border-left: 1px solid #fff;
			border-right: 1px solid #fff;
			text-align: center;
		}
		
		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}
    </style>
</head>

<body>

	<header>
		<h1>Embedding Models</h1>
		<nav>
			<a href="#overview">Overview</a> |
			<a href="#model">Choosing the Correct Model</a> |
			<a href="#semantic">Semantic Search</a> |
			<a href="#clustering">Clustering or Semantic Search</a> |
			<a href="#sentence_similarity">Sentence Similarity</a> |
			<a href="#rounded">Well Rounded</a> |
			<a href="#dimensions">Max Sequence and Dimensions</a> |
			<a href="#total_context">Total Program Context</a>
		</nav>
	</header>
	
	<main>
	
	<h2 id="overview">Overview</h2>
	
	<p>My program loads an embedding model into memory for:
<ol>
    <li>Creating the vector database</li>
    <li>Querying the vector database before your question and the "context" it obtains are sent to the LLM for an answer.</li>
</ol>
	
	<p><b>To get the most out of this program, it's crucial to choose the right embedding model.  Remember, the LLM's
	response is only as good as the context you provide via the embedding model.</b></p>

	<h2 id="model">Choosing the Correct Model</h2>
	
	<p>The first rule of embedding models is experiment.  Make sure and review the list of models displayed when you click
	"Download Model".  These instructions directly relate to the information displayed there. With that being said, there
	is no one-size-fits-all .  However, based my research and testing the following rules apply:</p>
	
	<h2 id="semantic">Semantic Search</h2>
	
	<p>Semantic search generally refers to obtaining results that are similar to your query.  This includes RAG and other
	tasks, but this program is geared towards RAG.  All of the embedding models with the "Semantic search" description
	will perform reasonably well for this with a few nuances.</p>
	
	<p>Models with <code>msmarco</code> in their name are specifically trained to provide longer results when
	given a short question:</p>
	
	<p><b>Tell me everything that this legal treatise say about the elements of a defamation claim?</b></p>
	<p><b>What are all the kinds of troubles that the main character from the book experiences?</b></p>
	
	<p>This is called "asymmetric semantic search.  Other models with the "Semantic search" description are GENERALLY
	"symmetric semantic search," which means that they tend to produce results of the same length of your question.</p>
	
	<p>In addition, models with <code>multi-qa</code> in their name have specifically been trained on question answering
	and are similar in function to <code>msmarco</code> models.  You'll just have to experiment with the different
	models as well as how you structure your query.</p>
	
	<p>With that being said, more recent larger models like ones beginning with <code>gtr</code> (to give one example)
	claim to be just as efficient as "specialist" models.  The recent trend has been for larger models good at everything.</p>
	
	<h2 id="clustering">Clustering or Semantic Search</h2>
	
	<p>Models with the description of "clustering or semantic search" basically means "well rounded."  They are good at
	question answering as well as other tasks, but for purposes of this program you should think of them as "well rounded."</p>
	
	<p>The <code>all-mpnet-base-v2</code> model is widely considered the best of its size in this category, and some claim
	that it performs better than models in the "Semantic Search" category, for example.	And as previously stated, the larger
	models might outperform, or vice versa.  Just experiment.</p>
	
	<h2 id="sentence_similarity">Sentence Similarity</h2>
	
	<p>There are only three models with this description.  They focus on providing sentences that are very similar to
	your question/sentence as possible; for example:</p>
	
	<p><b>Quote for me all sentences that discuss the main character in this book eating food.</b></p>
	<p><b>Provide me all sentences verbatim of a court discussing the elements of a defamation claim.</b></p>
	
	<p>The search results should be multiple chunks with highly relevant sentences (as opposed to answering a question):</p>
	
	<h2 id="rounded">Well Rounded</h2>
	
	<p>Self-explanatory.  But again, the larger models sometimes perform just as good as the specialist models.</p>
	
	<p>The "customizable" description means that you can modify an "instruction" parameter when running the model.  However, I
	removed this setting from the Settings tab to conserve space since I never saw the need based off of my searched.  If you
	want to add it back, feel free to modify <code>gui_tabs_settings_models.py</code> to make the setting visible again.
	Only the <code>instructor</code> and <code>bge</code> models use this parameter.</p>
	
	<h2 id="dimensions">Max Sequence and Dimensions</h2>
	
	<p>The "max sequence" of an embedding model refers to the maximum number of tokens (not characters) that it can process at
	once.  A model's "dimensions" refers to how nuanced a meaning it can extract from text.  A higher "dimensions" means that
	the model can discern nuanced meanings from very similar text and provide more accurate results.</p>
	
	<p>Within the "Settings" tab, you should set the chunk size to one-fourth (1/4) of the "max sequence" (it's OK to do less)
	of the embedding model that you plan to use.  This is because approximately four characters equals one (1) token.
	If the chunks you feed the embedding model are larger than its "max sequence" they will simply be truncated, meaning
	that you lose valuable data.  There will not be an error for you to notice this happening.  For example, if you set
	the chunk size to 1200, make sure that the embedding model you choose has a "max sequence" of at least 300.</p>
	
	<h2 id="total_context">Total Program Context</h2>
	
	<p>Finally, be mindful of the total context (in tokens) available from the LLM within LM Studio (typically 4096).
	My program prints in the command prompt the total number of tokens being sent to the LLM within LM Studio.
	To calculate how many tokens the LLM has left to provide a response, simply simply subtract it's maximum context
	length by how many tokens you sent it (plus your relatively small query itself).  For example, if your query and
	the contexts from the vector database equal 1000 tokens, the LLM will have 3096 tokens left to provide a response.</p>
	
	<p>You can then adjust the chunk size or the number of chunks to make sure the LLM has enough context to respond.</p>

</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
