<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embedding Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #161b22;
            color: #d0d0d0;
        }
		
		header {
			text-align: center;
			background-color: #3498db;
			color: #333;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}
		
		header a {
			color: #fff;
			text-decoration: none;
		}
		
		h2:target {
			padding-top: 200px;
			margin-top: -100px;
			display: block;
		}

		header a:hover {
			text-decoration: underline;
		}
		
		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}
		
		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}
		
		p {
			text-indent: 25px;
		}

		
		table {
            color: black;
			border-collapse: collapse;
            margin: 25px auto;
        }
		
		thead th {
			background-color: #f69784;
		}

        table, th, td {
            border: 1px solid black;
        }

        th, td {
            padding: 8px 12px;
        }

        .tiny {
            background-color: #e6f7ff;
        }

        .base {
            background-color: #b3e0ff;
        }

        .small {
            background-color: #66c2ff;
        }

        .medium {
            background-color: #3399ff;
        }

        .large {
            background-color: #0073e6;
        }
		
		code {
			background-color: #d0d0d0;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
		
		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		.table-from-second-file, .table-from-second-file th, .table-from-second-file td {
			border-collapse: collapse;
			margin: 25px auto;
			text-align: center;
			padding: 8px;
			border-bottom: 1px solid #ddd;
			color: #000;
		}

		.table-from-second-file th {
			background-color: #f2f2f2;
			color: #000;
		}
		
		.table-from-second-file td {
			color: #fff; /* White color for non-header cells */
			border-bottom: 1px solid #fff;
			border-left: 1px solid #fff;
			border-right: 1px solid #fff;
			text-align: center;
		}
		
		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}
    </style>
</head>

<body>

	<header>
		<h1>Embedding Models</h1>
		<nav>
			<a href="#overview">Overview</a> |
			<a href="#model">Choosing the Correct Model</a> |
			<a href="#semantic">Semantic Search</a> |
			<a href="#clustering">Clustering or Semantic Search</a> |
			<a href="#sentence_similarity">Sentence Similarity</a> |
			<a href="#rounded">Well Rounded</a> |
			<a href="#dimensions">Max Sequence and Dimensions</a> |
			<a href="#total_context">Total Program Context</a>
		</nav>
	</header>
	
	<main>
	
	<h2 id="overview">Overview</h2>
	
	<p>My program loads an embedding model into memory for:
<ol>
    <li>Creating the vector database</li>
    <li>Querying the vector database before your question and the "context" it obtains are sent to the LLM for an answer.</li>
</ol>
	
	<p><b>To get the most out of this program, it's crucial to choose the right embedding model.  Remember, the LLM's
	response is only as good as the context you provide it via the embedding model.</b></p>

	<h2 id="model">Choosing the Correct Model</h2>
	
	<p>The first rule of embedding models is experiment.  Make sure and review the list of models displayed when you click
	"Download Model".  These instructions directly relate to the information displayed there. With that being said, there
	is no one-size-fits-all .  However, based my research and testing the following rules apply:</p>
	
	<h2 id="semantic">Semantic Search</h2>
	
	<p>Semantic search is a general term that refers to obtaining results that are responsive to your query.  This includes results
	that are responsive to your question but also results that are similar to your question.  This obviously includes retrievel
	augmented generation (RAG), which this program is geared towards.  Semantic search models are nuanced.</p>
	
	<p>Models with <code>msmarco</code> in their name are specifically trained to provide longer results when
	given a short question.  This is called "asymmetric" semantic search because the results you expect are a lot
	longer than the question you pose.  For example:</p>
	
	<p><b>Tell me everything that this legal treatise say about the elements of a defamation claim?</b></p>
	<p><b>What are all the kinds of troubles that the main character from the book experiences?</b></p>
	
	<p>On the other hand, "symmetric semantic search" means that the results will tend to be a similar length.</p>
	
	<p>Models with <code>multi-qa</code> in their name have specifically been trained on question answering
	and are similar in function to <code>msmarco</code> models.  Experiement to see which models give the best results
	based off the style of text entered into the database.  Also, experiment with the structure of your query.</p>
	
	<p>Recent larger models like ones beginning with <code>gtr</code> (to give one example) claim to be just as efficient
	as "specialist" models.  The recent trend has been for larger models good at everything.</p>
	
	<h2 id="clustering">Clustering or Semantic Search</h2>
	
	<p>Models with the description of "clustering or semantic search" means they are good at question answering as
	well as other tasks, but for purposes of this program you should think of them as "well rounded."  The
	<code>all-mpnet-base-v2</code> model is widely considered the best of its size in this category, and some claim
	that it performs better than models in the "Semantic Search" category.</p>
	
	<h2 id="sentence_similarity">Sentence Similarity</h2>
	
	<p>There are only three models with this description, which focusese on providing sentences that are very similar to
	the sentece you pose; for example:</p>
	
	<p><b>Quote for me all sentences that discuss the main character in this book eating food.</b></p>
	<p><b>Provide me all sentences verbatim of a court discussing the elements of a defamation claim.</b></p>
	
	<p>The search results should be multiple chunks with highly relevant sentences (as opposed to answering a question).
	If you're using these sentences you should severaly reduce the chunk size since each "chunk" returned should be
	a sentence - e.g. a setting of 200 is a good place to start experimenting.</p>
	
	<h2 id="rounded">Well Rounded</h2>
	
	<p>Self-explanatory, but again, the larger models sometimes perform just as good as the specialist models.</p>
	
	<h2 id="dimensions">Max Sequence and Dimensions</h2>
	
	<p>The "max sequence" of an embedding model refers to the maximum number of <code>tokens</code> (not characters) that it can process at
	a time.  A model's "dimensions" refers to how nuanced a meaning it can extract from text.  A higher "dimensions" means that
	the model can discern nuanced meanings from very similar text and provide more accurate results.</p>
	
	<p>Within the "Settings" tab, you should set the chunk size to no more than one-fourth (1/4) of the "max sequence"
	(it's okay to do less) of the embedding model that you plan to use.  Approximately four characters equals one (1) token.
	Therefore, if you set the chunk size too large (e.g. 2000) you'll likely exceed the vector model's max sequence.
	This won't cause an error, but the chunk will simply be truncated, hampering your search results.  For example, if the vector
	model you're using has a max sequence of 300 try not to set the chunk size to more than 1,200.</p>
	
	<h2 id="total_context">Total Program Context</h2>
	
	<p>Remember that the LLM within LM Studio has a context length as well (also in tokens).  Most mainstream models are
	4096, which is plenty, but you can easily exceed this if you increase the number of "contexts" obtained from the
	vector database.  For example, if you set "contexts" at 20 and each context (i.e. "chunk") has a length of 500 characters,
	that's a total of approximately 2,500 tokens.  These tokens, along with your question, are sent to the LLM for a response
	and it'll have the remainder of 4096 to provide it.  Plan accordingly.</p>
	
	<p>Ideally, it's better to focus on crafting better queries, using an appropriate embedding model, and using an
	appropriate chunk size and overlap rather than the sheer number of contexts.  If become proficient, you should get a
	chunk with the relevant information in no more than the first 5-6 contexts.</p>
	
	<p>However, it's possible to use this program without LM Studio.  For example, if you're using one of the "Sentence Similarity"
	models described above.  In this instance, you may want 100 "contexts" returned and not need a response from the LLM.  You can
	click the "chunks only" checkbox to obtain this no problem.</p>
	
	<p>Another use case is creating a vector database of only images (or only searching images).  You may want any and all image
	chunks that relate to a certain topic.  You can use the "chunks only" checkbox and set the "contexts" to 1000+ for this.</p>

</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
