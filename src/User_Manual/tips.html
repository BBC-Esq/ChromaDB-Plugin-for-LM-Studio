<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tips</title>
	<style>
		body {
			font-family: Arial, sans-serif;
			line-height: 1.6;
			margin: 0;
			padding: 0;
			background-color: #161b22;
			color: #d0d0d0;
		}

		header {
			text-align: center;
			background-color: #3498db;
			color: #fff;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}

		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}

		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}

		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}

		p {
			text-indent: 35px;
		}

		table {
			border-collapse: collapse;
			width: 80%;
			margin: 50px auto;
		}

		th, td {
			text-align: left;
			padding: 8px;
			border-bottom: 1px solid #ddd;
		}

		th {
			background-color: #f2f2f2;
			color: #000;
		}

		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		code {
			background-color: #f9f9f9;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
	</style>

</head>

<body>
    <header>
        <h1>Usage Tips</h1>
    </header>

    <main>

        <section>
		
		<h2 style="color: #f0f0f0;" align="center">Database Searching</h2>
		
		<p>It is important to know how the <code>contexts</code>, <code>similarity</code>, and <code>search filter</code>
		settings work and interact.</p>

		<p><code>Similarity</code> refers to how relevant a chunk must be to a user's query before it's eligible to be returned.
		You can choose a number between zero and "1."  Prior versions of this program relied on <code>chromadb</code> and a higher
		number mean that it was more restrictive.  In other words, using a higher number required a higher relevance score for
		a chunk to be returned.  Since switching to <code>TileDB</code> in Version 4.3, however, a higher number now means
		the opposite.  In other words, specifying a higher number will reduce the relevant threshhold and allow more results
		to be returned.  A good rule of thumb is to use <code>.9</code> unless you are receiving too many irrelevant results.</p>
		
		<p><code>Contexts</code> refers to how many chunks of text will be returned.  This program will automatically choose
		the "X" most relevant contexts to return up to the number of contexts you want returned.</p>
		
		<p>The <code>contexts</code> setting is subject to the <code>similarity</code> setting.  In other words, even if you
		specify a higher number of contexts to return it'll only return up to that amount as long as each one meets the
		"similarity threshhold."  No chunks will be returned if none meet this threshhold.</p>
		
		<p>The <code>search filter</code> settings takes this step further and should be used carefully.  It filters the chunks
		that have already been subjected to the "contexts" and "similarity" settings.  It essentially searches each chunk for
		a keyword and removes any chunks that do not include that keyword verbatim.  And only those chunks are sent to LM
		Studio along with your question.  Please note, although it is "verbatim" it is not case-sensitive.  For example, if
		you use a search filter of "child," chunks with "children" or "child's" would still be excluded, but "Child" would not
		result in a chunk being excluded.</p>
		
		<p>A good approach to seeing how this setting works is to set the "similarity" setting very permissive and the "contexts"
		extremely high, and then experiment with different seach filter terms to see how the number of chunks changes.</p>
		
		<h2 style="color: #f0f0f0;" align="center">General VRAM Considerations</h2>
		
		<p>To conserve VRAM you can unplug any secondary monitors from the GPU and plug them into the graphics ports coming
		from your motherboard instead.  This forces your monitor to only use system memory and not VRAM.  However, your CPU
		must have "integrated graphics" for this to work.  Intel CPUs that have the letter "F" at the end do not contain
		integrated graphics, for example.</p>
		
		<p>Please note that some motherboards automatically disable the graphics adapters if a GPU is installed.  The
		motherboard manufacturer basically assumes that all monitors will be plugged into the GPU.  You can change
		this behavior by entering BIOS and re-enable the graphics ports coming from your motherboard.  Check your motherboard's
		documentation on how to do this because each BIOS is slightly different.</p>
		
		<h2 style="color: #f0f0f0;" align="left">Efficiently use VRAM</h2>
		
		<p>The most important thing is to load the entire LLM used in LM Studio into VRAM (not system RAM) when interacting
		with the LLM.  Even if you load 99% of the "layers" into VRAM, you will NOT get 99% of the speed compared to
		the entire model being in VRAM.</p>
		
		<p>Likewise, when you create the vector database you want to ensure that the entire embedding model is loaded into VRAM.
		If you don't have sufficient VRAM to have both the LLM within LM Studio and the embedding model in VRAM, this necessitates
		"ejecting" the LLM within LM Studio prior to creating the vector database.  After the database is created you can simply
		reload the LLM within LM Studio.</p>
		
		<p>Do not use GPU-acceleration when querying the vector database.  Querying the database requires 1,000x LESS
		(no exaggeration) compute power compared to creating the database.  Therefore, you can easily query the embedding model
		using your CPU (and hence not use VRAM).  This will ensure that the LLM within LM Studio is entirely within VRAM.</p>
		
		<p>If you don't have a GPU (and can't use VRAM) you're forced to manager your system memory.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Select an Appropriate Embedding Model</h2>
		
		<p>The new Embedding Models portion of this User Guide has a thorough explanation.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Select an Appropriate Whisper Model</h2>
		
		<p> The Transcribe portion of this User Guide has a more in-depth discussion.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Ensure Sufficient Context Length for the LLM</h2>
		
		<p>See the "Settings" portion of this User Manual for more information.  However, in general each embedding model has
		it's own unique token limit (referred to as "max sequence"), which can be seen by clicking the "Download" embedding
		model button.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Quality Transcriptions</h2>
		
		<p>It's generally recommended to use the large-v2 model for transcribing an audio file to a <code>.txt</code> file to
		be put into the vector database.  You can get away with using a smaller model when transcribing your query and can easily
		correct any errors in the question box, but you want a high-quality transcription when transcribing a file.  If you're
		short on VRAM, this necessitates ejecting the LLM from within LM Studio before transcribing a file so that the entire
		Whisper model will use VRAM.</p>
		
		<p>The Whisper model will be removed from VRAM immediately after transcribing a file, at which point you can simply reload
		the LLM within LM Studio.</p>

	</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
