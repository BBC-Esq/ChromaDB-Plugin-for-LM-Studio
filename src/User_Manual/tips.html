<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tips</title>
	<style>
		body {
			font-family: Arial, sans-serif;
			line-height: 1.6;
			margin: 0;
			padding: 0;
			background-color: #161b22;
			color: #d0d0d0;
		}

		header {
			text-align: center;
			background-color: #3498db;
			color: #fff;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}

		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}

		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}

		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}

		p {
			text-indent: 35px;
		}

		table {
			border-collapse: collapse;
			width: 80%;
			margin: 50px auto;
		}

		th, td {
			text-align: left;
			padding: 8px;
			border-bottom: 1px solid #ddd;
		}

		th {
			background-color: #f2f2f2;
			color: #000;
		}

		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		code {
			background-color: #f9f9f9;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
	</style>

</head>

<body>
    <header>
        <h1>Tips</h1>
    </header>

    <main>

        <section>
		
		<h2 style="color: #f0f0f0;" align="center">Manage VRAM</h2>
		
		<p>It's important to load the entire model from LM Studio into VRAM (or as many layers as possible), not system RAM.
		Even if you load 99% of the LM Studio model into VRAM with 1% into system RAM, you will NOT get 99% of the speed as if
		the ENTIRE model was loaded into VRAM.  My program additionally loads an "embedding" model to create/search the vector
		database and a Ctranslate2 Whisper model for the transcription functionality. Therefore, it is important manage your
		memory to achieve the best performance.</p>
		
		<p>If you are tight on VRAM, do not have LM Studio running while the vector database is being created.  I highly
		recommend that you choose "cpu" for when querying the database and only use "cuda" or "mps" when creating the database.
		Creating the database takes 5,000 times more compute power and it's worth using gpu-acceleration (and hence VRAM).
		However, merely querying the database can be done on any CPU easily and when you select "CPU" you're using system RAM
		and not VRAM.  The option to choose difference compute devices for database creation versus querying is a recent addition.
		
		<p>To save VRAM, unplug any secondary monitors from the GPU and plug them into graphics ports (e.g. HDMI or DisplayPort)
		coming directly from your motherboard.  This will prevent these monitors from using your GPU.  You will most likely want
		to keep your main monitor plugged in (e.g. for gaming).</p>
		
		<p>PLEASE NOTE, some motherboards automatically disable the motherboard's graphics adapters if a GPU is installed.
		It basically assumes that you will be connecting all monitors to your GPU.  If this occurs, you will need to enter BIOS
		and re-enable the graphics adapters on your motherboard.  The name of the specific setting can vary so check your specific
		motherboard's documentation.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Select an Appropriate Embedding Model</h2>
		
		<p>Previously, the <code>instructor</code> models performed best IMHO.  However, I have since corrected a mistake in
		my code and now recommend using the <code>BGE v1.5</code> models for 90% of use cases.  They perform just as good and
		use less memory.  Also, <code>all-mpnet-base-v2</code> is good and is low-memory.  Here are some resources to read:</p>
		
		<p><b>https://www.sbert.net/docs/pretrained_models.html</b></p>
		<p><b>https://instructor-embedding.github.io/</b></p>
		<p><b>https://github.com/FlagOpen/FlagEmbedding</b></p>
		<p><b>https://huggingface.co/thenlper/gte-large</b></p>
		<p><b>https://huggingface.co/jinaai/jina-embedding-l-en-v1</b></p>
		
		<h2 style="color: #f0f0f0;" align="center">Select the Appropriate Model Within LM Studio</h2>
		
		<p>My program uses the embedding model to create the database and subsequently obtain "context" from it, which is
		then forwarded to the LLM within LM Studio along with your question, for an answer.
		The embedding model (not the LLM) is responsible for the quality of the context and it is overwhelmingly this quality
		that determines the quality of the answer you get from LM Studio.  Therefore, if VRAM is short, prioritize a higher
		quality embedding model over a larger LLM.  Even a 7B model quantized to 8-bit can be overkill.</p>
		
		<p>This is the smallest model that still works decently IMHO, but my current overall favorite is Mistral:</p>
		
		<p><b>https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF</b></p>
		
		<p>There's one caveat, if the documents in your vector database are highly technical (e.g. medical or legal documents),
		a larger LLM might provide some benefit because of its increased vocabulary.  Just experiment.</p>
		
		<p>Also, remember that my program only supports llama-based models that follow the llama "prompt format."</p>
		
		<h2 style="color: #f0f0f0;" align="center">Select the Appropriate Transcription Model and Quantization</h2>
		
		<p> The transcription functionality uses state-of-the-art Whisper models converted to the Ctranslate2 format.
		These models are designed to excel at even poor quality audio with people taking overr on another, and transcribing
		your question is relatively easy. Therefore, try a smaller transcription model or lower quantization to save VRAM.
		Also, you can run transcription model to run on your CPU (and hence use RAM versus VRAM).
		Detailed instructions are located within the "Whisper" tab within the GUI.</p>
		
		</section>
		<section>
		
		<h2 style="color: #f0f0f0;" align="center">Load LM Studio After Creating Database</h2>
		
		<p>The embedding model can use gpu-acceleration as well, and creating the database uses more memory than merely
		querying it; therefore, don't load a model into LM Studio until after creating the database.
		This will reduce the chance that you run out of VRAM when creating the database.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Ask the Right Questions</h2>
		
		<p>Modify your question if  you don't get a good answer.  Sometimes there's a big difference between
		"What is the statute of limitations for defamation?" versus "What is the statute of limitations for a defamation
		action if the allegedly defamatory statement is in writing as opposed to verbal?"  Experiment with how specific you are.</p>
		
		<p>My previous advice was to not ask multiple questions, but now that I've added an option to increase the number of
		"contexts" from the database to the LLM, this is less stringent.  I now encourage you ask longer-winded questions and even
		general descriptions of the types of information you're looking for (not strictly a question you see).  For reference, here
		are my prior instructions:</p>
		
		<p><i>Don't use multiple questions.  For example, the results will be poor if you ask "What is the statute of limitations for a
		defamation action?" AND "Can the statute of limitations tolled under certain circumstances?" at the same time.  Instead,
		reformulate your question into something like: "What is the statute of limitations for a defamation and can it be tolled
		under certain circumstances?"  Again, just experiment and DO NOT assume that you must use a larger LLM or embedding model.</i></p>
		
		<h2 style="color: #f0f0f0;" align="center">Ensure Sufficient Context Length for the LLM</h2>
		
		<p>Rarely, the server log within LM Studio might give you an error stating that the context is too long.  Increase the maximum
		"context" of the LLM within LM Studio.  The default is 1500 but most models nowadays accept up to 4096.
		I've limited my program to providing only 4 chunks of "context" and have never seen this go over 4096.</p>

		<h2 style="color: #f0f0f0;" align="center">Ensure Sufficient Context for the Embedding Model</h2>
		
		<p>It is currently set at 512 "tokens" per chunk, which is more than enough for 99% of use cases.  A setting to adjust
		this is coming soon, but feel free to contact me if 512 isn't providing sufficient context.</p>

	</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
