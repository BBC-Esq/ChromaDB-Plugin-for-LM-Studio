<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tips</title>
	<style>
		body {
			font-family: Arial, sans-serif;
			line-height: 1.6;
			margin: 0;
			padding: 0;
			background-color: #161b22;
			color: #d0d0d0;
		}

		header {
			text-align: center;
			background-color: #3498db;
			color: #fff;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}

		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}

		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}

		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}

		p {
			text-indent: 35px;
		}

		table {
			border-collapse: collapse;
			width: 80%;
			margin: 50px auto;
		}

		th, td {
			text-align: left;
			padding: 8px;
			border-bottom: 1px solid #ddd;
		}

		th {
			background-color: #f2f2f2;
			color: #000;
		}

		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		code {
			background-color: #f9f9f9;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
	</style>

</head>

<body>
    <header>
        <h1>Tips</h1>
    </header>

    <main>

        <section>
		
		<h2 style="color: #f0f0f0;" align="center">Manage VRAM</h2>
		
		<p>It's important to load the entire model from LM Studio into VRAM (or as many layers as possible), not system RAM.
		Even if you load 99% of the LM Studio model into VRAM with 1% into system RAM, you will NOT get 99% of the speed as if
		the ENTIRE model was loaded into VRAM.  My program additionally loads an "embedding" model to create/search the vector
		database and a Ctranslate2 Whisper model for the transcription functionality. Therefore, it is important manage your
		memory to achieve the best performance.</p>
		
		<p>If you are tight on VRAM, do not have LM Studio running while the vector database is being created.  I highly
		recommend that you choose "cpu" for when querying the database and only use "cuda" or "mps" when creating the database.
		Creating the database takes 5,000 times more compute power and it's worth using gpu-acceleration (and hence VRAM).
		However, merely querying the database can be done on any CPU easily and when you select "CPU" you're using system RAM
		and not VRAM.  The option to choose difference compute devices for database creation versus querying is a recent addition.
		
		<p>To save VRAM, unplug any secondary monitors from the GPU and plug them into graphics ports (e.g. HDMI or DisplayPort)
		coming directly from your motherboard.  This will prevent these monitors from using your GPU.  You will most likely want
		to keep your main monitor plugged in (e.g. for gaming).</p>
		
		<p>PLEASE NOTE, some motherboards automatically disable the motherboard's graphics adapters if a GPU is installed.
		It basically assumes that you will be connecting all monitors to your GPU.  If this occurs, you will need to enter BIOS
		and re-enable the graphics adapters on your motherboard.  The name of the specific setting can vary so check your specific
		motherboard's documentation.</p>
		
		<h2 style="color: #f0f0f0;" align="center">See The User Guide Embedding Models Button</h2>
		
		<p>See the new Embedding Models portion of the User Guide.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Select the Appropriate Transcription Model and Quantization</h2>
		
		<p> The transcription functionality uses state-of-the-art Whisper models converted to the Ctranslate2 format.
		These models are designed to excel at even poor quality audio with people taking overr on another, and transcribing
		your question is relatively easy. Therefore, try a smaller transcription model or lower quantization to save VRAM.
		Also, you can run transcription model to run on your CPU (and hence use RAM versus VRAM).
		Detailed instructions are located within the "Whisper" tab within the GUI.</p>
		
		</section>
		<section>
		
		<h2 style="color: #f0f0f0;" align="center">Load LM Studio After Creating Database</h2>
		
		<p>The embedding model can use gpu-acceleration as well, and creating the database uses more memory than merely
		querying it; therefore, don't load a model into LM Studio until after creating the database.
		This will reduce the chance that you run out of VRAM when creating the database.</p>
		
		<h2 style="color: #f0f0f0;" align="center">Ensure Sufficient Context Length for the LLM</h2>
		
		<p>Rarely, the server log within LM Studio might give you an error stating that the context is too long.  Increase the maximum
		"context" of the LLM within LM Studio.  The default is 1500 but most models nowadays accept up to 4096.
		I've limited my program to providing only 4 chunks of "context" and have never seen this go over 4096.</p>

		<h2 style="color: #f0f0f0;" align="center">Ensure Sufficient Context for the Embedding Model</h2>
		
		<p>It is currently set at 512 "tokens" per chunk, which is more than enough for 99% of use cases.  A setting to adjust
		this is coming soon, but feel free to contact me if 512 isn't providing sufficient context.</p>

	</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
