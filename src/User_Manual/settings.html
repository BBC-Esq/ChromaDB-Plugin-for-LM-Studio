<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Settings</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background-color: #161b22;
      color: #d0d0d0;
    }

    header {
      text-align: center;
      background-color: #3498db;
      color: #fff;
      padding: 20px;
      position: sticky;
      top: 0;
      z-index: 999;
    }

    main {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }

	h1 {
	  color: #333;
	}

	h2 {
	  color: #f0f0f0;
	  text-align: center;
	}

    p {
      text-indent: 35px;
    }

    table {
      border-collapse: collapse;
      width: 80%;
      margin: 50px auto;
    }

    th, td {
      text-align: left;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }

    th {
      background-color: #f2f2f2;
      color: #000;
    }

    footer {
      text-align: center;
      background-color: #333;
      color: #fff;
      padding: 10px;
    }
    
    code {
      background-color: #f9f9f9;
      border-radius: 3px;
      padding: 2px 3px;
      font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
      color: #333;
    }
  </style>
</head>
<body>

  <header>
    <h1>Settings</h1>
  </header>

  <main>
    <h2>Server/LLM Settings</h2>
    <p>The <code>port</code> number in these settings must match the one you've set in LM Studio. If you update it in LM
	Studio, make sure to update it here as well.</p>
	
	<p>The <code>max-tokens</code> setting is <code>-1</code> by default, which allows the LLM to provide a response
	that is unlimited in length.  Most off the time the LLM will cut itself off after sufficiently answering your
	question; however, rarely it will repeat itself or ramble.  Therefore, you can change this setting if need be.
	Remember, any number here besides <code>-1</code> is in tokens (not characters).</p>

    <h3>Temperature Setting</h3>
    <p>The <code>temperature</code> setting can be between 0 and 1, and determines the creativity of the LLM's response.
	Zero means don't be creative.</p>

    <h3>Prefix and Suffix</h3>
    <p>The <code>prefix</code> and <code>suffix</code> settings are tailored for LLAMA 2-based models by default, and
	it also works pretty well with <code>Mistral</code> models.  Do not change this setting unless you know what you're
	doing.  Since you just need a basic LLM to answer questions based on the context from the vector database, I
	recommend using basic models like Llama-2 itself or Mistral.</p>
	
	<p>Within LM Studio, you need to turn OFF the Automatic Prompt Formatting setting within the server tab in order
	for the program to work best.  However, you can disable the prefix/suffix setting within this program by clicking
	the "disable" checkbox, just make sure to re-enable the setting in LM Studio and know what you're doing.</p>

    <h2>Database Settings</h2>
    <p>The <code>chunk size</code> and <code>chunk overlap</code> settings apply to Langchain's
	"RecursiveCharacterTextSplitter," which is responsible for splitting the text before it's entered into the
	vector database.  These settings are in CHARACTERS not TOKENS.</p>

	<p>How large the chunks are and whether there is an overlap has a direct impact on the quality of
	the results received from the vector database.  Feel free to experiment with different settings to see if
	it improves the search results.  However, make sure that the chunk size falls under the "token" limit of the embedding
	model you use.  Different embedding models have different token limits (like different LLM's do).  </p>
	
	<p>A token is approximately four (4) characters.  For example, if you set the chunk size to 1,200, make sure the
	embedding modle you choose has a maximum token limit of at least 300.</p>
	
	<p>Ultimately, you must leave enough "context" (in tokens) for the LLM to provide a response.  You can calculate it
	like this: <code>all chunks + your question + LLM's response</code> should fall within the LLM's token context limit
	(usually 4096).  If what you send the LLM exceeds 4096 you will get an error message, and even if you don't, the
	LLM may cut itself off if it doesn't have enough context to provide a sufficient answer (no error message for this).</p>
	
	<h2>Whisper Settings</h2>
	
	<p> Whisper models are used throughout this program to transcribe your question for the LLM as well as transcribe an
	audio file to put it into the database.  See the User Guide section on this for more details.  Generally, you should
	transcribe your question using CPU and only use GPU acceleration to transcribe an audio file.  If VRAM is especially
	a concern, unload the model from LLM Studio and load it back after the transcription is completed.  Both uses of
	Whisper models remove the model immediately after their done being used in order to conserve VRAM.</p>
	
	<h2>Test Embeddings</h2>
	
	<p>The setting is useful to actually see the "contexts" provided by the vector database.  Checking this box will
	obtain and display the contexts, and no longer connect to LM Studio.  This is useful for fine-tuning your chunk size
	and overlap and other settings before connecting to LM Studio.</p>
	
	<h2>Break in Case of Emergency</h2>
	<p>All settings for this progrma are keps in <code>config.yaml</code>.  If you accidentally change a setting you don't
	like or this file gets deleted or corrupted, there's a backup cpoy inside the "User Guide" folder.</p>
    
  </main>

  <footer>
    <p>www.chintellalaw.com</p>
  </footer>

</body>
</html>
