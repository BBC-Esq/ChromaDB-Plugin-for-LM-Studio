<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Settings</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background-color: #161b22;
      color: #d0d0d0;
    }

    header {
      text-align: center;
      background-color: #3498db;
      color: #fff;
      padding: 20px;
      position: sticky;
      top: 0;
      z-index: 999;
    }

    main {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }

	h1 {
	  color: #333;
	}

	h2 {
	  color: #f0f0f0;
	  text-align: center;
	}

    p {
      text-indent: 35px;
    }

    table {
      border-collapse: collapse;
      width: 80%;
      margin: 50px auto;
    }

    th, td {
      text-align: left;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }

    th {
      background-color: #f2f2f2;
      color: #000;
    }

    footer {
      text-align: center;
      background-color: #333;
      color: #fff;
      padding: 10px;
    }
    
    code {
      background-color: #f9f9f9;
      border-radius: 3px;
      padding: 2px 3px;
      font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
      color: #333;
    }
  </style>
</head>
<body>

  <header>
    <h1>Settings</h1>
  </header>

  <main>
	<h2><u>Server/LLM Settings</u></h2>
    <p><code>Port</code>: Must match the port being used in LM Studio.</p>
	
	<p><code>Max-tokens</code>: The maximum number of tokens that the LLM can use when providing a response. The default
	of <code>-1</code> allows the LLM an unlimited number (up to the model's maximum context length specified within LM Studio).</p>

    <p><code>Temperature</code>: Determines the creativity of the LLM's response.  Can be between 0 and 1.  Zero means
	don't be creative at all.</p>

    <h3>Prompt Format</h3>
    <p>A <code>Prefix</code> and <code>Suffix</code> are sent to LM Studio along with your question and the results from the
	vector database search.  Different LLM's are trained with different "prompt formats" and your results
	will degrade if you don't use the correct one.</p>

	<p>I recommend using a model within LM Studio that matches a present provided in this program since I've tested the models
	specifically for RAG, but there's also space to enter a custom prefix/suffix if you want to try different models.  Make sure
	to clear the prefix and suffix boxes as well as disable "automatic prompt formatting" within LM Studio as there
	have been bug reports if you don't.</p>
	
	<p><code>Disable</code>: Disables prompt formatting so you can exclusively use the prompt formatting settings within LM Studio.</p>
	
	<h2><u>Database Creation</u></h2>
	
	<p>Before text is put into the vector database (whether it originates from a pdf, audio transcription or image description) it
	must be broken into chunks by the "Recursive Character Text Splitter" (aka, "Splitter") created by Langchain so that when you search the database you can get relevant chunks/contexts.  The chunks of text are then converted into vectors by the embedding model and
	put into the vector database.</p>
	
	<p><code>Chunk Size</code>: The maximum length of a chunk/context in characters (NOT tokens).  The Splitter will strive to achieve this limit. but may create shorter chunks by splitting between paragraphs sentences to keep related ideas together.</p>

	<p><code>Chunk Overlap</code>:Specifies the maximum number of characters (NOT tokens) that each chunk will include from the
	previous chunk.  For example, if <code>chunk overlap</code> is set to 250 then first 250 characters of "Chunk #2" will consist
	of the last 250 characters of "Chunk #1."  The goal is to try and preserve meaning if, for example, text is split in the
	middle of a sentence.</p>
	
	<p>A good rule of thumb is to set <code>chunk overlap</code> to 1/4 or 1/3 of the <code>chunk size</code>.</p>
	
	<h2><u>Database Query</u></h2>
	<p><code>Create Device</code>: Automatically populated with "cpu", "cuda" or "mps" based on your hardware.  Always use
	"cuda" or "mps" if available.</p>
	
	<p><code>Contexts</code>: The maximum number of chunks/contexts that can be returned.  This is an upper limit, however,
	and a chunk will only be returned if its meets the <code>similarity</code> threshhold setting.  For example, a setting of "10"
	will return the 10 most relevant chunks/contexts (provided they all meet the similarity setting).</p>
	
	<p><code>Similarity</code>: The threshhold of how relevant a chunk/context must be before it will be returned.  A setting
	of <code>.1</code> is a low-relevance while .9 would be very high.  Do not use "1."</p>
	
	<p><code>Search Filter</code>: After chunks have been obtained, excludes a chunk/context unless it contains the specified
	search term VERBATIM (but is not case-sensitive).  This filter only runs AFTER the <code>contexts</code> and <code>similarity</code> settings.</p>

	For example, if the filter is "book" then only chunks that contain "book" would be sent to the LLM or displayed (if "chunks only" is checked).  Even chunks with terms like "books", "bookend," or similar derivations would still be excluded, but terms like "Book,"
	"book" or "bOoK" would not result in the chunk being excluded.</p>
	
	<p><code>Clear Filter</code>: Removes any filters.
	
	<p><code>All Files</code>, <code>Images-Only</code>, <code>Documents-Only</code>, and <code>Audio-Only</code>:  Specifies which
	types of files will be searched within the vector database.  For example, if you choose <code>Audio-Only</code>, only
	chunks/contexts that originate from an audio file will be returned (subject to the similarity and filter settings of course).</p>
	
	<p><code>Chunks Only</code> checkbox:  If checked, LM Studio will not be connected to and the chunks received from the vector database will be displayed instead.  This is extremely useful to test the chunk size/overlap settings as well as the quality of the
	vector model you are using.</p>
	
	<h2><u>Bark Settings</u></h2>
	
	<p>The "Bark Response" button will convert the LLM's response to audio.  I only recommend using this feature if you have gpu-acceleration and I've disabled it for certain platforms due to compatibility issues until I find a better solution.</p>
	
	<p><code>Size</code>: Size of the Bark model to use, each with its own resource requirements, speed, and accuracy.</p>
	
	<p><code>Quant</code>: <u>float32</u> casts the bark model in its original floating point format.  <u>float16</u> cuts the
	memory/compute requirements by half with a relatively small loss in quality.</p>
	
	<p><code>Better Transformer</code>: Use the "better transformers" feature created by Huggingface, which provides a
	general 5-20% speedup.  This should always be checked unless you experience problems running the Bark model.</p>

	<p><code>Cpu_Offload</code>: The Bark model actually consists of four sub-models (<code>semantic</code>, <code> coarse</code>, <code>fine</code>, and <code> encodec</code>), which are run sequentially to convert text to audio.  If unchecked, all four sub-models will be loaded into VRAM until the Bark model is done processing.  If checked, only the sub-model being used will reside in VRAM and the other three will be put into system RAM.  This results in a small delay in processing and is not recommended unless you are running short on VRAM.  If you are not using gpu-acceleration (not recommended) this setting has no effect since all four sub-models
	will always be in system RAM.</p>
	
	<p><code>Voices</code>: Choose different voices.  <code>v2/en_speaker_6</code> is the highest quality IMHO but it's fun to
	experiment.  The only female voice is <code>v2/en_speaker_9</code>.
	<h2><u>Vision Models</u></h2>
	
	<p>Consult the "Vision" tab within the User Guide.</p>
	
	<h2>Break in Case of Emergency</h2>
	<p>Settings for this program are stored in <code>config.yaml</code>.  If you mistakenly change a setting or lose this file,
	there is a backup within the <u>User Guide</u> folder.</p>
    
  </main>

  <footer>
    <p>www.chintellalaw.com</p>
  </footer>

</body>
</html>
