<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Settings</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background-color: #161b22;
      color: #d0d0d0;
    }

    header {
      text-align: center;
      background-color: #3498db;
      color: #fff;
      padding: 20px;
      position: sticky;
      top: 0;
      z-index: 999;
    }

    main {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }

	h1 {
	  color: #333;
	}

	h2 {
	  color: #f0f0f0;
	  text-align: center;
	}

    p {
      text-indent: 35px;
    }

    table {
      border-collapse: collapse;
      width: 80%;
      margin: 50px auto;
    }

    th, td {
      text-align: left;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }

    th {
      background-color: #f2f2f2;
      color: #000;
    }

    footer {
      text-align: center;
      background-color: #333;
      color: #fff;
      padding: 10px;
    }
    
    code {
      background-color: #f9f9f9;
      border-radius: 3px;
      padding: 2px 3px;
      font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
      color: #333;
    }
  </style>
</head>
<body>

  <header>
    <h1>Settings</h1>
  </header>

  <main>
    <h2>Server/LLM Settings</h2>
    <p>The <code>port</code> number in these settings must match the one you've set in LM Studio. If you update it in LM
	Studio, make sure to update it here as well.</p>
	
	<p>The <code>max-tokens</code> setting is <code>-1</code> by default, allows the LLM to provide a response that is
	unlimited in length.  99% it will cut itself off after sufficiently answering your question, however, so there's
	little risk in using <code>-1</code>.  However, you can change it to experiment.  Remember, any number here besides
	<code>-1</code> is in tokens (not characters).</p>

    <h3>Temperature Setting</h3>
    <p>The <code>temperature</code> setting can be between 0 and 1, and it determines the creativity of the LLM's response.
	Zero means don't be creative.</p>

    <h3>Prefix and Suffix</h3>
    <p>The <code>prefix</code> and <code>suffix</code> settings are tailored for LLAMA 2-based models by default, and
	this also works with <code>Mistral</code> models.  Do not change this setting unless you're 100% sure about the
	prompt format that a model needs to function efficiently.  Since you just need a basic LLM to answer questions from
	context you provide, stick with basic models like Llama-2 itself of Mistral, but make sure that the model uses the
	Llama-2 prompt format.</p>

    <h2>Database Settings</h2>
    <p>The <code>chunk size</code> and <code>chunk overlap</code> settings apply to Langchain's
	"RecursiveCharacterTextSplitter," which is responsible for splitting the text before it's entered into the
	vector database.  In short, this program extracts text, chunks it, and then sends the chunks to the embedding
	model, which then puts it into the vector database.  Feel free to experiment with different chunk sizes to see if
	it improves the search results.  However, make sure that the chunk size falls under the "token" limit of the embedding
	model you use.  Different embedding models have different token limits (like different LLM's do).</p>
	
	<p>The "chunk" size setting is in the number of characters (not tokens), and one token is approximately four characters.
	Therefore, if you set the chunk size to 1,200, for example, make sure the embedding modle you choose has a maximum
	token limit of at least 300.</p>

    <h3>Chunk Size</h3>
    <p>The RecursiveCharacterTextSplitter tries to create chunks of the specified size, but it adheres to certain criteria
	as to when it can split chunks.  the specified chunk size as possible. However, it adheres to certain cutoff points
	such as the end of a paragraph. As such, your text might be split in the middle of two ideas/concepts that are
	related.  That's where the "overlap" setting comes in.</p>
	
	<p>The "chunk overlap" setting (also in characters, not tokens) starts the next chunk to include the specified number
	of characters of the former chunk so no meaning is lost (ideally).  Feel free to experiment with this setting as well to
	improve the search results that are fed to the LLM for an answer.  The most important thing to remember, however, is to
	keep the chunk size within the embedding model's token limit, and make sure to leave enough overall context for the LLM
	to provide a sufficient response.</p>
	
	<p>You can calculate it like this: <code>all chunks + your question + LLM's response</code> should fall within the LLM's token
	context limit (usually 4096).  If what you send the LLM exceeds 4096 you will get an error message, and even if you don't,
	the LLM may cut itself off if it doesn't have enough context to provide a sufficient answer (no error message for this).</p>
	
	<p>On average, there are four characters per "token" Therefore, if you set the chunk size to 1,200 characters that equals
	approximately 300 tokens...and if you requst 12 "contexts" from the database, that equals 3,600 tokens, whihc leaves the
	LLM approximatelyk 496 tokens to provide a response.  This is usually sufficient, but it might not be...just experiment.</p>
	
	<h2>Whisper Settings</h2>
	
	<p> Whisper models are used throughout this program to transcribe your question for the LLM as well as the new feature to
	transcribe an audio file to put it into the database.  See the User Guide section on this for more details.  Generally,
	however, you should transcribe your question using CPU and only use GPU acceleration to transcribe an audio file.  If
	VRAM is short when transcribing an audio file, unload the model from LLM Studio and load it back after the transcription
	is completed.  Both utilizations of Whisper models remove the model immediately after their done being used in order to
	conserve valuable VRAM.</p>
	
	<h2>Break in Case of Emergency</h2>
	<p>All settings for this progrma are keps in <code>config.yaml</code>.  If you accidentally change a setting you don't
	like or this file gets deleted or corrupted, there's a backup cpoy inside the "User Guide" folder.</p>
    
  </main>

  <footer>
    <p>www.chintellalaw.com</p>
  </footer>

</body>
</html>
