<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Settings</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background-color: #161b22;
      color: #d0d0d0;
    }

    header {
      text-align: center;
      background-color: #3498db;
      color: #fff;
      padding: 20px;
      position: sticky;
      top: 0;
      z-index: 999;
    }

    main {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }

	h1 {
	  color: #333;
	}

	h2 {
	  color: #f0f0f0;
	  text-align: center;
	}

    p {
      text-indent: 35px;
    }

    table {
      border-collapse: collapse;
      width: 80%;
      margin: 50px auto;
    }

    th, td {
      text-align: left;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }

    th {
      background-color: #f2f2f2;
      color: #000;
    }

    footer {
      text-align: center;
      background-color: #333;
      color: #fff;
      padding: 10px;
    }
    
    code {
      background-color: #f9f9f9;
      border-radius: 3px;
      padding: 2px 3px;
      font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
      color: #333;
    }
  </style>
</head>
<body>

  <header>
    <h1>Settings</h1>
  </header>

  <main>
	<h2><u>Server/LLM Settings</u></h2>
    <p><code>Port</code>: Must match the port that you've set in LM Studio.</p>
	
	<p><code>Max-tokens</code>: Refers to the maximum amount of tokens that the LLM can use to provide a response.
	The default is <code>-1</code>, which allows the LLM an unlimited amount of tokens (up to the maximum context length
	specified within LM Studio).  This setting is mostly useful for smaller models that sometimes ramble.</p>

    <p><code>Temperature</code>: Determines the creativity of the LLM's response.  Can be between 0 and 1.  Zero means
	don't be creative at all.  The default setting is <code>.1</code>.</p>

    <h3>Prompt Format</h3>
    <p><code>Prefix</code> and <code>Suffix</code>: refer to what precedes and follows what you send to the LLM.
	LLM's are trained with a certain "prompt format" and they provide the best responses when your questions are
	structured to fit this prompt format.  For example, Llama-2-chat uses the <code>[INST]</code> and <code>[/INST]</code>
	prefix and suffix, respectively.</p>

	<p>I've included a few presets for models that I think provide the best responses for RAG, but there's also space to enter
	a custom prefix/suffix.  Make sure and check the model card on Huggingface for the proper format if you don't use a preset.</p>
	
	<p> Also, my program does not utilize "system messages" or other complex prompt formats.  It only uses "prefix" and "suffix," which
	I've found more than sufficient for RAG purposes.  However, if you want to use an LLM that can accommodate more complex prompt
	formatting, click the <code>disable</code> button in my program and enable "Automatic Prompt Formatting" within LM Studio.</p>
	
	<h2><u>Voice Recorder Settings</u></h2>
	
	<p> This program uses state-of-the-art Whisper models to (1) transcribe your question and (2) transcribe audio files to
	<code>.txt</code>, which are then put into the vector database.  Details about these settings are further explained in the
	<u>Whisper Tab</u> of this User Guide.</p>

	<p>In general, however, I recommend transcribing your question using CPU and using GPU acceleration to transcribe an audio file.
	Moreover, when transcribing a file using gpu-acceleration I recommend using as large of a Whisper model as possible and no less
	than float16/bfloat16 quantization.  If need be, eject the LLM from within LM Studio when transcribing a file to free up VRAM.
	The selectable options are automatically determined by what's compatible with your CPU and/or GPU.</p>
	
	<h2><u>Bark Settings</u></h2>
	
	<p>These settings pertain to the <u>Bark Response</u> button, which plays audio of the LLM's response using "Bark" models.
	You must wait for the LLM's entire response to be received otherwise it may give an error.  DO NOT use this feature
	unless: (1) you have an Nvidia GPU; (2) you are using an AMD GPU, running Linux, and have installed all the proper
	acceleration software (e.g. RocM); or (3) have Apple MPS.  Otherwise, it'll revert to CPU-usage and be EXTREMELY SLOW.</p>
	
	<p><code>Size</code>: Setting refers to either the normal size Bark model or the small model, each with its own resource
	requirements, speed, and accuracy.  The small model is used by default.</p>
	
	<p><code>Quant</code>: Allows a user to choose the original <u>float32</u> version of the Bark model or the <u>float16</u>
	quantized version.  Using <u>float16</u> saves significant resources with little quality loss, but feel free to try both!</p>
	
	<p><code>Better Transformer</code>: Better Transformer is a library created by Huggingface that provides a general 5-20% speedup.
	Since it is compatible with most systems, you should leave this checked unless you're getting strange errors.</p>

	<p><code>Cpu_Offload</code>: Bark actually consists of four sub-models and only one is used at any given moment.  Checking
	this setting makes it so only the active sub-model is using VRAM while the others are kept in system RAM.  However, this
	noticeably decreases speed.  This setting is unchecked by default and you should leave it that way unless you are extremely
	short on VRAM.</p>
	
	<p><code>Voices</code>: Allows a user to test Barks premade voices.  <code>v2/en_speaker_6</code> is the default setting
	and the highest quality in my opinion, but feel free to experiment.  The only female voice is <code>v2/en_speaker_9</code>.
	Eventually this program might incorporate voices able to speak in different languages, which will be around the same time
	that the functionality to translate to/from different languages is added:</p>
	
	<p>https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c</p>
	
	<h2><u>Database Settings</u></h2>
    <p><code>Chunk Size</code> and <code>Chunk Overlap</code>: These two settings refer to how the "RecursiveCharacterTextSplitter"
	splits the text that you extract from the various documents you want to put into the vector database.  These settings are in
	"characters," NOT tokens.  The chunks of text are then vectorized by the embedding model and put into the database.  Feel free to
	experiment with different settings.  Different settings might work better with different styles of text.</p>
	
	<p>However, make sure that the chunk size does not exceed the <code>max sequence</code> of the embedding model you are using.
	Each embedding model has a <code>max sequence</code>, which you can view by clicking the "<u>Download Embedding Model</u>" button.
	This limit determines the maximum length (in tokens, NOT characters) that the model can process at a time.  Thus, if you
	choose a "chunk size" of 1,200, for example, make sure the embedding model you choose has a <code>max sequence</code> of
	at least 300.  This is based off of the rule of thumb that a token is approximately 4 characters.</p>
	
	<p><code>Contexts</code>: Refers to how many "chunks" you want to retrieve and send to the LLM for a response.  Make sure
	that the number of contexts, multiplied by the "chunk size," doesn't exceed the LLM's maximum context limit set within
	LM Studio.  Also, make sure and leave enough context for the LLM to provide a response.</p>

	<p>For example, if the LLM's limit is 4096 and you send 10 "contexts" each having 1,200 characters (i.e. 300 tokens) you've
	already used up 3,000 tokens of the LLM's 4096 limit.  This would leave 1096 tokens for the LLM's response, which is usually
	(but not always) sufficient to answer your question.</p>
	
	<p><code>Chunks Only</code>: Checking this checkbox will return only the relevant chunks of text from the vector database and
	will not connect to the LLM.  This is extremely useful to text the chunk size/overlap settings to ensure that you're getting
	relevant chunks before connecting to the LLM.</p>
	
	<h2>Break in Case of Emergency</h2>
	<p>Settings for this program are stored in <code>config.yaml</code>.  If you mistakenly change a setting or lose this file,
	there is a backup within the <u>User Guide</u> folder.</p>
    
  </main>

  <footer>
    <p>www.chintellalaw.com</p>
  </footer>

</body>
</html>
