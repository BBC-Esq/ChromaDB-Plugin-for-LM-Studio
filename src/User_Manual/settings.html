<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Settings</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background-color: #161b22;
      color: #d0d0d0;
    }

    header {
      text-align: center;
      background-color: #3498db;
      color: #fff;
      padding: 20px;
      position: sticky;
      top: 0;
      z-index: 999;
    }

    main {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }

	h1 {
	  color: #333;
	}

	h2 {
	  color: #f0f0f0;
	  text-align: center;
	}

    p {
      text-indent: 35px;
    }

    table {
      border-collapse: collapse;
      width: 80%;
      margin: 50px auto;
    }

    th, td {
      text-align: left;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }

    th {
      background-color: #f2f2f2;
      color: #000;
    }

    footer {
      text-align: center;
      background-color: #333;
      color: #fff;
      padding: 10px;
    }
    
    code {
      background-color: #f9f9f9;
      border-radius: 3px;
      padding: 2px 3px;
      font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
      color: #333;
    }
  </style>
</head>
<body>

  <header>
    <h1>Settings</h1>
  </header>

  <main>
    <h2>Server/LLM Settings</h2>
    <p>The <code>port</code> number in these settings must match the one you've set in LM Studio. If you update it in LM
	Studio, make sure to update it here as well.</p>
	
	<p>The <code>max-tokens</code> setting is <code>-1</code> by default, which allows the LLM to provide an unlimited
	response (up to its maximum context length).  Most off the time the LLM will automatically stop after sufficiently
	answering your question.  However, sometimes smaller models ramble or repease themselves and you can styumie that
	that behavior with this setting.  Remember, any number other than <code>-1</code> is in tokens (not characters).</p>

    <h3>Temperature Setting</h3>
    <p>The <code>temperature</code> setting can be between 0 and 1.  Zero means don't be creative at all.  The default
	setting is <code>1</code> becuase typically with RAG tasks you simply want factual information.</p>

    <h3>Prefix and Suffix</h3>
    <p>The <code>prefix</code> and <code>suffix</code> settings are <code>[INST]</code> and <code>[/INST]</code> by default,
	which is the prompt format for <code>Llama2-chat</code> and <code>Mistral</code> models, but NOT necessarily fine-tuned
	versions of those models by third parties.  I recommend using the original <code>Mistral</code> (7B) or <code>Llama2-chat</code>
	(7B or 13B) models with this program.  RAG primarily involves summarizing the contexts received from the vector database,
	not providing creative answers that fine-tuned models are good at.  However, using fine-tuned models can be beneficial
	if the type of text in the database is highly technical (e.g. medical terminology).</p>
	
	<p>The pulldown menu contains the basic "prefix" and "suffix" for Mistral/Llama2, Neural-Chat, Orca2, and ChatML formats.
	Selecting one will populate the boxes to enter the prefix and suffix, thus, you must still click the update settings button.</p>
	
	<p>However, feel free to experiment.  If you want to control the prompt formatting within this program, however, you
	need to disable the automatic prompt formatting within LM Studio.  If you don't want this program to handle prompt
	formatting simply check the "disable" checkbox, but make sure to enable prompt formatting within LM Studio.</p>

    <h2>Database Settings</h2>
    <p>The <code>chunk size</code> and <code>chunk overlap</code> settings refer to where the "RecursiveCharacterTextSplitter,"
	splits the text you provide it in order to create "chunks" that will be entered into the database.  These settings are
	in CHARACTERS, not TOKENS.</p>

	<p>The chunk size and overlap directly impact the quality of the results received from the vector database.  Feel free to
	experiment with different settings, however, because different settings might work better with different styles of text.
	For example, different settings might work better for a book than an EXCEL spreadsheet.</p>
	
	<p>However, make sure that the chunk size falls under the "max sequence" limit of the embedding being used to create
	the vector database.  Different embedding models have max sequence limits (in tokens, not characters), and if you feed it
	chunks larger than its max sequenced it will simply truncate the chunk.  This won't produce an error, but you will lose
	valuable data.  You can seethe various max sequence limits of the various models by clicking the "Download" button.</p>
	
	<p>Also, remember that the chunk settings are in characters (no tokens) and that there are approximatelky four (4) characters
	per token.  Therefore, if you set the chunk size to 1,200, the embedding model's max sequence should be at least 300.</p>
	
	<p>The <code>Contexts</code> setting refers to how many chunks you want returned from the vector database.  A greater number
	obviously means more context for the LLM to use as its knowledge base.  However, it also means that more of the LLM's maximum
	context length will be available for it to provide a response.</p>
	
	<p>To summarize, <code>all chunks from vector database + your question + LLM's response</code> should fall within the LLM's
	token context limit (usually 4096).  If what you send the LLM exceeds 4096 you will get an error message.  If what
	you send the LLM does NOT exceed the total context limit, but the LLM's response would, the LLM will simply cut itself
	off and you'll get an incomplete response.</p>
	
	<h2>Whisper Settings</h2>
	
	<p> Whisper models are used throughout this program to transcribe your question for the LLM as well as transcribe an
	audio file to a <code>.txt</code> file, to put it into the database folder.  Generally, you should
	transcribe your question using CPU and only GPU acceleration to transcribe an audio file and use as large of a Whisper
	model that you can.  If VRAM is especially limited, eject LLM from LLM Studio and load it back after the transcription
	is complete to ensure that the entire Whisper model is loaded into VRAM and not system memory.</p>
	
	<h2>Test Embeddings</h2>
	
	<p>Checking the <code>test embeddings</code> checkbox will NOT query the LLM and simply return the chunks separated by a
	chunk delimeter so you can see the quality of the chunks.  This is useful to test various chunk settings as well as different
	embedding models.  Once you're sure that you're getting quality chunks, uncheck this box.</p>
	
	<h2>Break in Case of Emergency</h2>
	<p>All settings for this progrma are keps in <code>config.yaml</code>.  If you accidentally change a setting you don't
	like or this file gets deleted or corrupted, there's a backup cpoy inside the "User Guide" folder.</p>
    
  </main>

  <footer>
    <p>www.chintellalaw.com</p>
  </footer>

</body>
</html>
