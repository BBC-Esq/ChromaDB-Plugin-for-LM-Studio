<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Floating Point Format and Quantization </title>
	<style>
		body {
			font-family: Arial, sans-serif;
			line-height: 1.6;
			margin: 0;
			padding: 0;
			background-color: #161b22;
			color: #d0d0d0; 
		}

		header {
			text-align: center;
			background-color: #3498db;
			color: #fff;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}

		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}

		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}

		h1, h2 {
			color: #333;
		}

		p {
			text-indent: 35px;
		}

		table {
			border-collapse: collapse;
			width: 80%;
			margin: 50px auto;
		}

		th, td {
			text-align: left;
			padding: 8px;
			border-bottom: 1px solid #ddd;
		}

		th {
			background-color: #f2f2f2;
			color: #000;
		}

		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
	</style>

</head>

<body>
    <header>
        <h1>Floating-Point Formats</h1>
    </header>

    <main>
        <section>
            <img src="./float.png" alt="Floating Point">
        </section>

        <section>
        <h2 style="color: #f0f0f0;">Introduction to Floating-Point Formats</h2>
        <p>Running an embedding or a large language model requires a lot of math calculations and computers don't understand
		decimals (1,2,3) like you and me.  Rather, they represent numbers with a series of ones and zeros called "bits."
		In general, the more bits used means higher quality but also higher VRAM/RAM and compute power needed.
		With that being said, the quality also depends on how many of the bits are "exponent" versus "fraction."</p>
		
		<p>The phrase "Floating point format" refers to the total number of bits used and how many are  "exponent" versus "fraction."
		The three most common floating point formats are shown above.  Notice that both Float16 and Bfloat16 use 16 bits but
		a different number of "exponent" versus "fraction" bits.</p>
		
		<p>"Exponent" bits essentially determine the "range" of numbers that a neural network can use when doing math.
		For example, Float32 has 8 "exponent" bits so hypothetically this allows the neural network to use any integer
		between one and one-hundred - its "range is 1-100.  Bfloat16 would have the same "range" because it also has
		8 "exponent" bits.  However, since Float16 only has 5 "exponent" bits its "range" might only be 1-50.</p>

        <p></p>
		
		<p>"Fraction" bits essentially determine the number of unique values that can be used within that "range."
		 For example, Float32 has 23 "fraction" bits so hypothetially it can use every whole number between 1-100 when doing math.
		Since Bfloat16 only has 7 "fraction" bits, it might only have 25 unique values within 1-100.
		This is also referred to as the "precision" of a neural network.</p>

        <p>These are hypotheticals and the actual ranges and precisions are summarized in this table:</p>
		
            <table border="1">
                <tr>
                    <th>Floating Point Format</th>
                    <th>Range (Based on Exponent)</th>
                    <th>Discrete Values (Based on Fraction)</th>
                </tr>
                <tr>
                    <td>float32</td>
                    <td>~3.4×10<sup>38</sup></td>
                    <td>8,388,608</td>
                </tr>
                <tr>
                    <td>float16</td>
                    <td>±65,504</td>
                    <td>1,024</td>
                </tr>
                <tr>
                    <td>bfloat16</td>
                    <td>~3.4×10<sup>38</sup></td>
                    <td>128</td>
                </tr>
            </table>
			
        </section>

		<p>The "range" and "precision" both determine the "quality" of an output, but in different ways.
		In general, different floating point formats are good for different purposes.  For example, Google, which created
		Bfloat16, found that it was better for neural networks while Float16 was better for scientific calculations.</p>
		
		<p>You can see the floating point format of the various embedding models used in my program by looking at the
		"config.json" file for each model.</p>
		
        <section>
            <h2 style="color: #f0f0f0;">What is Quantization?</h2>
			
            <p>"Quantization" refers to converting the original floating point format to one with a smaller "range" and "precision."
			Projects like LLAMA.CPP and AutoGPTQ do this with slightly different algorithms.  The overall goal is to reduce
			the memory and computational power needed while only suffering a "reasonable" loss in quality.
			Specific "quantizations" like "Q8_0" or "8-bit" refer to the "floating point format" of "int8."
			(Technically, "int8" is no longer "floating" but you don't need to delve into the nuances of this to understand
			the basic concepts I'm trying to communicate.)</p>
			
			<p>Here is the range and precision for "Int8," which is clearly less:</p>

            <table border="1">
                <tr>
                    <th>Floating Point Format</th>
                    <th>Range (Based on Exponent)</th>
                    <th>Discrete Values (Based on Fraction)</th>
                </tr>
                <tr>
                    <td>int8</td>
                    <td>-128 to 127</td>
                    <td>±127 (within integer range)</td>
                </tr>
            </table>
        </section>

	</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
