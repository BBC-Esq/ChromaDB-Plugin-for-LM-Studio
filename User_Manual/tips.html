<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tips</title>
	<style>
		body {
			font-family: Arial, sans-serif;
			line-height: 1.6;
			margin: 0;
			padding: 0;
			background-color: #333; /* Dark grey background */
			color: #f5f5f5; /* Off-white text color */
		}

		header {
			text-align: center;
			background-color: #3498db;
			color: #fff;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}

		main {
			max-width: 800px;
			margin: 0 auto;
			padding: 20px;
		}

		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}

		h1, h2 {
			color: #333;
		}

		p {
			text-indent: 35px; /* Indent paragraphs */
		}

		table {
			border-collapse: collapse;
			width: 100%;
		}

		th, td {
			text-align: left;
			padding: 8px;
			border-bottom: 1px solid #ddd;
		}

		th {
			background-color: #f2f2f2;
			color: #000;
		}

		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		code {
			background-color: #f9f9f9; /* light gray background */
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
	</style>

</head>

<body>
    <header>
        <h1>Tips</h1>
    </header>

    <main>

        <section>
        <h2 style="color: #f0f0f0;" align="center">Manage VRAM</h2>
        <p>My program loads an "embedding" model to create/search the vector database and Ctranslate2 Whisper model for the
		transcription functionality.  Therefore, when you use LM Studio, there are three models loaded into memory.
		Therefore, it is important to manage your memory, whether it's RAM or VRAM to achieve the best performance.  This is
		especially true if you are using VRAM.</p>
		
		<p>If you're using gpu-acceleration, it's crucial to load the entire model from LM Studio into VRAM (or as
		many layers as possible) and not system RAM.  This greatly increases speed.  The pathways for sytem RAM and
		PciExpress are "relatively" slow compared to VRAM.  Even if you load 99% of the LM Studio model into VRAM with 1% into system
		RAM, you will NOT get 99% of the speed as if the ENTIRE model was loaded into VRAM.  Therefore, follow these tipes
		to try and achieve this:</p>
		
		<h3 style="color: #f0f0f0;">Tip #1 - Multiple Monitors</h3>
		
		<p>If you have more than one monitor connected to your graphics card, move them to your onboard graphics instead.
		You can do this by unplugging any secondary monitors from the GPU and plugging them intographics ports
		(e.g. HDMI or DisplayPort) coming directly from your motherboard.  This will prevent these monitors from using the VRAM
		on your graphics card.  You will most likely need to keep your main monitor plugged into your GPU (e.g. for gaming high
		framerates)...but other monitors can be safely reallocated.</p>
		
		<p>For example, I have a 4k monitor connected to my GPU and a 1080p monitor connected to my motherboard.  This frees
		up approximately 1-1.5 GB of VRAM on my graphics card.  PLEASE NOTE, however, that some motherboards automatically
		disable the motherboard's graphics adapters if it detects a separate GPU is installed.  In other words, it
		basically presumes that all monitors will be connectec to your GPU.  If this occurs, you will need to enter BIOS
		and "enable" the graphics adapters coming from your motherboard.  What the specific setting is called can vary depending
		on the motherboard manufacturer; therefore, check motherboard documentation.</p>
		
		<h3 style="color: #f0f0f0;">Tip #2 - Ctranslate2/Whisper Transcription Models</h3>
		
		<p> Ctranslate2/Whisper models are state-of-the-art and very high quality.  Even the "tiny" model will usually work
		for transcribing a simple qustion...unless your audio quality is very poor.  These models are extremely good at
		transcribing low quality audio, people talking over one another, etc., so transcribing your simple question is easy.
		Therefore, if VRAM is tight, try a smaller model.  Smaller = less VRAM required.</p>
		
		<p>Instructions to change the Ctranslate2/Whisper model can be found in the "Whisper" tab within the GUI.</p>
		
		<p>Also, you can specify the transcription model to run on your CPU.  This should not create a meaningful delay.  Again,
		we are not transcribing hours of low quality audio here...To make this change, simply modify <code>line 13</code> of
		<code>voice_recorder_module.py</code> from "auto" to "cpu."  Moreover, you can change the default number of cpu
		threads that is uses from 8 to whatever your CPU has.  Remember, 8 does NOT have quotation marks around it.</p>
		 
		<p>This makes the transcription model run purely on CPU (hence system RAM) and not VRAM, thus freeing up more VRAM for
		the LM Studio model.  If using the <code>small.en</code> this will save you 1-2 GB of VRAM.
		
		<h3 style="color: #f0f0f0;">Tip #3 - Selecting an Appropriate Embedding Model</h3>
		
		<p>Previously, the <code>instructor</code> models were the best IMHO.  However, I had accidentally omitted the
		"instructions" for the <code>BGE</code> models, which has now been corrected.  I now recommend using the
		<code>BGE v1.5</code> models for 90% of use cases.  They perform just as good as the "instruct" models but with
		slightly less VRAM.  Also, do not overlook <code>"all-mpnet-base-v2"</code>.  It requires much less VRAM than
		the and instructor and BGE models and is specifically trained for stereotypical text passages.  Before trying models
		other than instructor/BGE/all-mpnet, I highly recommend that you search the read a
		little <a href=https://www.sbert.net/docs/pretrained_models.html> HERE</a> about the different kinds of models.</p>
		
		<h3 style="color: #f0f0f0;">Tip #4 - Selecting a Small Model Within LM Studio</h3>
		
		<p>My program basically sends your question along with "context" to LM Studio for an answer.  This is essentially
		how "retrieval augmented generation" works.  A lot of people mistakenly assume that it's the LLM that performs the
		search or somehow interacts with the vector database when it is actuallyk the embedding model tha does all the
		heavy lifting.  The embedding model is responsible for creating the vector database, converting your question
		into a "vector" and pulling the relevant "context" from the database, and sending it to LM Studio.  LM Studio
		merely receives the question + context and responds, which is a "relatively" simple task.  Therefore, you do
		not need a high quality model within LM Studio.  By "high quality" I mean a 13billion parameter model...or a
		even a 7-billion parameter model using 8-bit quantization (e.g. Q8_0).  Instead, test 7-billion parameter at 4-bit
		quantization or lower to save VRAM.</p>
		
		<p>There is an exception, however.  If the documents in your vector database are highly technical such as medical
		studies or legal documents, for example, a higher-quality model within LM Studio might provide some benefit because
		of its increased vocabulary.  Overall, just experiment.</p>
		
		<p>Also, remember that my program only supports llama-based models.  What I mean by "llama-based models" is, models
		that strictly following the llama "prompt format."  This shouldn't be a hinderance because, again, you only need
		a basic model to work with this program.</p>
		
		<p>In contrast, for general chat continue to experiement with larger models or different "fine-tuned" models
		based on your use case (e.g. role-playing or coding)...but for purposes of this program, you only need a basic model.</p>
		
		</section>
		<section>
		
		<h2 style="color: #f0f0f0;" align="center">Program Usage</h2>
		
		<h3 style="color: #f0f0f0;">Tip #1 - Load LM Studio After Creating Database</h3>
		
		<p>Creating the database typically uses more memory than merely querying it.  Therefore, if the LM Studio model is
		loaded while you're creating the database you run the risk of VRAM spilling over into RAM, thus slowing the
		database creation.  I recommend loading LM Studio after creating the database.</p>
		
		<h3 style="color: #f0f0f0;">Tip #2 - Slightly Modify Questions</h3>
		
		<p>Slightly modify your question if  you don't get a good answer.  For example, there can be a big difference between
		"What is the statute of limitations for a defamation?" versus "What is the statute of limitations for a defamation
		action if the allegedly defamatory statement is in writing as opposed to verbal?"  Experiment with how specific
		your questions are.</p>
		
		<h3 style="color: #f0f0f0;">Tip #3 - Compound Questions</h3>
		
		<p>Vector databases do not work well with multiple questions.  If you ask "What is the statute of limitations for a
		defamation action?" AND "Can the statute of limitations tolled under certain circumstances?" in the same question,
		the results will be poor.  Instead, try reformulating your question into: "What is the statute of limitations
		for a defamation and can it be tolled under certain circumstances?"  This simple change will have drastic improvements.
		Again, just experiment with your question.  DO NOT assume that poor results always mean you need an embedding model
		that requires more VRAM.</p>
		
		<h3 style="color: #f0f0f0;">Tip #4 - Ensure Sufficient Context</h3>
		
		<p>As explained above, transmits your question with relevant "context" to LM Studio for an answer.  If you get an error
		stating that the context is too long, increase the maximum "context" of the model within LM Studio.  It's exceedingly
		rare, but sometimes the question + "context" will exceed the context limit of the LM Studio model.  However, I have yet
		to get this particular error when the context is set to 4096.</p>

	</main>

    <footer>
        <nav><a href="http://www.chintellalaw.com" target="_blank">www.chintellalaw.com</a></nav>
    </footer>
</body>
</html>
